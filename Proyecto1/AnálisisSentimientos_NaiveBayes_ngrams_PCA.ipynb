{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwN1T2dG-P39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098d28c0-d094-4d28-9ccc-be9bee6de178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk   \n",
        "import spacy            \n",
        "import re     \n",
        "import string            \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import twitter_samples    # Corpus Twitter\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rf1sRZeM3Iz"
      },
      "source": [
        "Lectura de Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBrs994RL_OH",
        "outputId": "c2dffa63-bac6-4674-e25c-3448d0faa8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j31D_0MDA2",
        "outputId": "8004c539-2e9d-4418-d38d-e9ec563ee769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive tweets:  5000\n",
            "Negative tweets:  5000\n"
          ]
        }
      ],
      "source": [
        "pos_tweets = twitter_samples.strings('positive_tweets.json') #tweets positivos\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json') #tweets negativos\n",
        "\n",
        "print(\"Positive tweets: \", len(pos_tweets))\n",
        "print(\"Negative tweets: \", len(neg_tweets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-An61QZMyWy"
      },
      "source": [
        "Procesamiento\n",
        "\n",
        "\n",
        "1. LowerCase\n",
        "2. Lematización / Stemming\n",
        "3. Remover stopword\n",
        "4. Remover signos de puntuación\n",
        "4. Remover urls y manejadores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ucdJQkMDM786"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(nlp):\n",
        "    special_cases = {\":)\": [{\"ORTH\": \":)\"}], \":(\": [{\"ORTH\": \":(\"}]}\n",
        "    simple_url_re = re.compile(r'''^https?://''')\n",
        "    suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\n",
        "    prefixes = nlp.Defaults.prefixes + [r'^[\\-\\—\\–\\+\\+\\.\\!\\/\\,\\\"\\(\\)\\[\\]\\{\\}\\:\\;\\<\\>\\?\\¿\\¡\\|\\&\\#\\@\\$\\%\\^\\*\\_\\\\\\'\\`\\~]']\n",
        "    suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
        "    prefixes_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "    return spacy.tokenizer.Tokenizer(nlp.vocab, rules=special_cases, suffix_search=suffix_regex.search, prefix_search=prefixes_regex.search, url_match=simple_url_re.match)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVAU5JDMPhIx"
      },
      "outputs": [],
      "source": [
        "def normalization(data, regularization=\"lemma\", language='english'):\n",
        "  stopwords = nltk.corpus.stopwords.words(language)\n",
        "  ps = PorterStemmer()\n",
        "  normalized_data = []\n",
        "  \n",
        "  for tweet in data:\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # identificar retweets\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet) #eliminar links\n",
        "    tweet = re.sub(r'#', '', tweet) #eliminar símbolo gato\n",
        "    tweet = re.sub(r'@\\w+', '', tweet) #eliminar palabras que inicias con @\n",
        "    tweet = re.sub(r'\\d+', '', tweet) #eliminar números\n",
        "    tweet = re.sub(' +', ' ', tweet) #quitar espacios\n",
        "\n",
        "    if regularization == \"stem\":\n",
        "      tweetTokenizer = TweetTokenizer()\n",
        "      words = tweetTokenizer.tokenize(tweet)\n",
        "      tokens = [ps.stem(w) for w in words]\n",
        "    if regularization == \"lemma\":\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.lemma_ for token in doc]\n",
        "    else:\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.text for token in doc]\n",
        "    \n",
        "    normalized_tweets = [w for w in tokens if w not in stopwords and not w==' ' and w not in string.punctuation]\n",
        "    normalized_data.append(normalized_tweets)\n",
        "  return normalized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWYzJu9XM789"
      },
      "source": [
        "Create Vocabulary and frequency dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AcHXwUxUM78-"
      },
      "outputs": [],
      "source": [
        "norm_pos = normalization(pos_tweets)\n",
        "norm_neg = normalization(neg_tweets)\n",
        "all_tweets = norm_pos + norm_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Os0j3U5kM78_"
      },
      "outputs": [],
      "source": [
        "def n_grams(words:list, n_gram:int):\n",
        "  if int(n_gram) == 1: return words\n",
        "  return [tuple(words[i:i+int(n_gram)]) for i,w in enumerate(words) if i <= (len(words)-int(n_gram))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2ES8_2M78_",
        "outputId": "88bd3d6a-2ca6-4a45-ce05-a7c5df2322be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The vocabulary has 40958 2-grams.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ngrams = 2\n",
        "n_grams_tweets = [n_grams(tweet, ngrams) for tweet in all_tweets]\n",
        "at = [w for tweet in n_grams_tweets for w in tweet]\n",
        "fd = nltk.FreqDist(at)\n",
        "vocabulary = sorted(list(fd.keys()))\n",
        "\n",
        "print('\\nThe vocabulary has ' + str(len(vocabulary)) + ' ' + str(ngrams) + '-grams.\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDL5vJalM79B",
        "outputId": "45d444bc-14f6-4b52-e04f-31bcd4042264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_features matrix has m = 10000 examples (rows).\n",
            "\n",
            "and  n = 40959 features (columns).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_features = []\n",
        "for text in all_tweets:\n",
        "  vector = [1] # initialize in 1 ?\n",
        "  for voc in vocabulary:\n",
        "    # In vector saves a list of vocabulary's length. \n",
        "    # Iterate each vocabulary word and count in each text list\n",
        "    vector.append(text.count(voc))\n",
        "  X_features.append(vector)\n",
        "\n",
        "print('\\nX_features matrix has m = %d examples (rows).\\n' %len(X_features))\n",
        "print('and  n = %d features (columns).\\n' %len(X_features[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XenIOd7aM79B"
      },
      "outputs": [],
      "source": [
        "tags = [1]*len(pos_tweets) + [0]*len(neg_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wiSx8TdUM79C"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCtuGEWYM79D",
        "outputId": "f3ea0483-105c-4e4b-d49c-a13fb93b1b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in true_divide\n",
            "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=1000)\n",
        "pca.fit(X_features)\n",
        "X_features_pca = pca.transform(X_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDWi68qyM79D",
        "outputId": "86a0f86c-97e7-4ef3-9c45-fe0f8acac715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perdida de información es de : nan %\n"
          ]
        }
      ],
      "source": [
        "#print(pca.explained_variance_ratio_)\n",
        "n_components = 1000\n",
        "suma = np.sum(pca.explained_variance_ratio_[:n_components])\n",
        "print(\"Perdida de información es de : \" + str(round(1-suma,4)) + \" %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D1ix11Q3M79E"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e9mYSzSFM79N"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_features_pca,tags,test_size=0.2, random_state=50)\n",
        "target_names = ['class 0', 'class 1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI1K0bumM79O",
        "outputId": "acb06f9b-4dd0-4b24-f09d-79bce0ab64ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.4995\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.0000    0.0000    0.0000      1001\n",
            "     class 1     0.4995    1.0000    0.6662       999\n",
            "\n",
            "    accuracy                         0.4995      2000\n",
            "   macro avg     0.2497    0.5000    0.3331      2000\n",
            "weighted avg     0.2495    0.4995    0.3328      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador de Naive Bayes\n",
        "modelo = MultinomialNB()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modelo.fit(X_train, y_train)\n",
        "predicciones = modelo.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modelo.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m8pW1p5M79O",
        "outputId": "2961c275-894a-4c78-a1a2-f688aeac0c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.4995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.0000    0.0000    0.0000      1001\n",
            "     class 1     0.4995    1.0000    0.6662       999\n",
            "\n",
            "    accuracy                         0.4995      2000\n",
            "   macro avg     0.2497    0.5000    0.3331      2000\n",
            "weighted avg     0.2495    0.4995    0.3328      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador de Naive Bayes\n",
        "modelo = BernoulliNB()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modelo.fit(X_train, y_train)\n",
        "predicciones = modelo.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modelo.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}