{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwN1T2dG-P39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk   \n",
        "import spacy            \n",
        "import re     \n",
        "import string            \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import twitter_samples    # Corpus Twitter\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rf1sRZeM3Iz"
      },
      "source": [
        "Lectura de Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBrs994RL_OH",
        "outputId": "e8d6b5c8-909f-4cc2-ac81-2fc555f30a01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading twitter_samples: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j31D_0MDA2",
        "outputId": "97f38f63-e161-4ce0-b25a-1c3dd5ad618c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive tweets:  5000\n",
            "Negative tweets:  5000\n"
          ]
        }
      ],
      "source": [
        "pos_tweets = twitter_samples.strings('positive_tweets.json') #tweets positivos\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json') #tweets negativos\n",
        "\n",
        "print(\"Positive tweets: \", len(pos_tweets))\n",
        "print(\"Negative tweets: \", len(neg_tweets))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F-An61QZMyWy"
      },
      "source": [
        "Procesamiento\n",
        "\n",
        "\n",
        "1. LowerCase\n",
        "2. Lematización / Stemming\n",
        "3. Remover stopword\n",
        "4. Remover signos de puntuación\n",
        "4. Remover urls y manejadores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_tokenizer(nlp):\n",
        "    special_cases = {\":)\": [{\"ORTH\": \":)\"}], \":(\": [{\"ORTH\": \":(\"}]}\n",
        "    simple_url_re = re.compile(r'''^https?://''')\n",
        "    suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\n",
        "    prefixes = nlp.Defaults.prefixes + [r'^[\\-\\—\\–\\+\\+\\.\\!\\/\\,\\\"\\(\\)\\[\\]\\{\\}\\:\\;\\<\\>\\?\\¿\\¡\\|\\&\\#\\@\\$\\%\\^\\*\\_\\\\\\'\\`\\~]']\n",
        "    suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
        "    prefixes_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "    return spacy.tokenizer.Tokenizer(nlp.vocab, rules=special_cases, suffix_search=suffix_regex.search, prefix_search=prefixes_regex.search, url_match=simple_url_re.match)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVAU5JDMPhIx"
      },
      "outputs": [],
      "source": [
        "def normalization(data, regularization=\"lemma\", language='english'):\n",
        "  stopwords = nltk.corpus.stopwords.words(language)\n",
        "  ps = PorterStemmer()\n",
        "  normalized_data = []\n",
        "  \n",
        "  for tweet in data:\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # identificar retweets\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet) #eliminar links\n",
        "    tweet = re.sub(r'#', '', tweet) #eliminar símbolo gato\n",
        "    tweet = re.sub(r'@\\w+', '', tweet) #eliminar palabras que inicias con @\n",
        "    tweet = re.sub(r'\\d+', '', tweet) #eliminar números\n",
        "    tweet = re.sub(' +', ' ', tweet) #quitar espacios\n",
        "\n",
        "    if regularization == \"stem\":\n",
        "      tweetTokenizer = TweetTokenizer()\n",
        "      words = tweetTokenizer.tokenize(tweet)\n",
        "      tokens = [ps.stem(w) for w in words]\n",
        "    if regularization == \"lemma\":\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.lemma_ for token in doc]\n",
        "    else:\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.text for token in doc]\n",
        "    \n",
        "    normalized_tweets = [w for w in tokens if w not in stopwords and not w==' ' and w not in string.punctuation]\n",
        "    normalized_data.append(normalized_tweets)\n",
        "  return normalized_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Vocabulary and frequency dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The vocabulary has 12428 words.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "norm_pos = normalization(pos_tweets)\n",
        "norm_neg = normalization(neg_tweets)\n",
        "all_tweets = norm_pos + norm_neg\n",
        "at = [w for tweet in all_tweets for w in tweet]\n",
        "fd = nltk.FreqDist(at)\n",
        "vocabulary = sorted(list(fd.keys()))\n",
        "\n",
        "print('\\nThe vocabulary has %d words.\\n' %len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "X_features matrix has m = 10000 examples (rows).\n",
            "\n",
            "and  n = 12428 features (columns).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_features = []\n",
        "for text in all_tweets:\n",
        "  vector = [] # initialize in 1 ?\n",
        "  for voc in vocabulary:\n",
        "    # In vector saves a list of vocabulary's length. \n",
        "    # Iterate each vocabulary word and count in each text list\n",
        "    vector.append(text.count(voc))\n",
        "  X_features.append(vector)\n",
        "\n",
        "print('\\nX_features matrix has m = %d examples (rows).\\n' %len(X_features))\n",
        "print('and  n = %d features (columns).\\n' %len(X_features[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tags = [1]*len(pos_tweets) + [0]*len(neg_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_features,tags,test_size=0.2, random_state=50)\n",
        "target_names = ['class 0', 'class 1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntaje:  0.953\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9495    0.9570    0.9532      1001\n",
            "     class 1     0.9566    0.9489    0.9528       999\n",
            "\n",
            "    accuracy                         0.9530      2000\n",
            "   macro avg     0.9530    0.9530    0.9530      2000\n",
            "weighted avg     0.9530    0.9530    0.9530      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con DecisionTree\n",
        "modeloDT = DecisionTreeClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloDT.fit(X_train, y_train)\n",
        "predicciones = modeloDT.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloDT.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntaje:  0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9694    0.9500    0.9596      1001\n",
            "     class 1     0.9509    0.9700    0.9604       999\n",
            "\n",
            "    accuracy                         0.9600      2000\n",
            "   macro avg     0.9602    0.9600    0.9600      2000\n",
            "weighted avg     0.9602    0.9600    0.9600      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con RandomForest\n",
        "modeloRF = RandomForestClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloRF.fit(X_train, y_train)\n",
        "predicciones = modeloRF.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloRF.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntaje:  0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9733    0.9461    0.9595      1001\n",
            "     class 1     0.9474    0.9740    0.9605       999\n",
            "\n",
            "    accuracy                         0.9600      2000\n",
            "   macro avg     0.9603    0.9600    0.9600      2000\n",
            "weighted avg     0.9604    0.9600    0.9600      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con SVC\n",
        "modeloSVC = SVC(kernel='linear', random_state=42)\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloSVC.fit(X_train, y_train)\n",
        "predicciones = modeloSVC.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloSVC.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntaje:  0.9595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9863    0.9321    0.9584      1001\n",
            "     class 1     0.9355    0.9870    0.9605       999\n",
            "\n",
            "    accuracy                         0.9595      2000\n",
            "   macro avg     0.9609    0.9595    0.9595      2000\n",
            "weighted avg     0.9609    0.9595    0.9595      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con AdaBoostClassifier\n",
        "modeloADA = AdaBoostClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloADA.fit(X_train, y_train)\n",
        "predicciones = modeloADA.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloADA.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntaje:  0.9435\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9431    0.9441    0.9436      1001\n",
            "     class 1     0.9439    0.9429    0.9434       999\n",
            "\n",
            "    accuracy                         0.9435      2000\n",
            "   macro avg     0.9435    0.9435    0.9435      2000\n",
            "weighted avg     0.9435    0.9435    0.9435      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con KNeighborsClassifier\n",
        "modeloKN = KNeighborsClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloKN.fit(X_train, y_train)\n",
        "predicciones = modeloKN.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloKN.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
