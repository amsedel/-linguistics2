{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwN1T2dG-P39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c9b040-a5e7-4631-d340-c5910cae16e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk   \n",
        "import spacy            \n",
        "import re     \n",
        "import string            \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import twitter_samples    # Corpus Twitter\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rf1sRZeM3Iz"
      },
      "source": [
        "Lectura de Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBrs994RL_OH",
        "outputId": "7f132680-d7c9-49e3-941e-9dd47258331a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j31D_0MDA2",
        "outputId": "6e6d6f0e-aa73-4271-edd3-271de2a880eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive tweets:  5000\n",
            "Negative tweets:  5000\n"
          ]
        }
      ],
      "source": [
        "pos_tweets = twitter_samples.strings('positive_tweets.json') #tweets positivos\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json') #tweets negativos\n",
        "\n",
        "print(\"Positive tweets: \", len(pos_tweets))\n",
        "print(\"Negative tweets: \", len(neg_tweets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-An61QZMyWy"
      },
      "source": [
        "Procesamiento\n",
        "\n",
        "\n",
        "1. LowerCase\n",
        "2. Lematización / Stemming\n",
        "3. Remover stopword\n",
        "4. Remover signos de puntuación\n",
        "4. Remover urls y manejadores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R_53bCA0IsyJ"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(nlp):\n",
        "    special_cases = {\":)\": [{\"ORTH\": \":)\"}], \":(\": [{\"ORTH\": \":(\"}]}\n",
        "    simple_url_re = re.compile(r'''^https?://''')\n",
        "    suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\n",
        "    prefixes = nlp.Defaults.prefixes + [r'^[\\-\\—\\–\\+\\+\\.\\!\\/\\,\\\"\\(\\)\\[\\]\\{\\}\\:\\;\\<\\>\\?\\¿\\¡\\|\\&\\#\\@\\$\\%\\^\\*\\_\\\\\\'\\`\\~]']\n",
        "    suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
        "    prefixes_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "    return spacy.tokenizer.Tokenizer(nlp.vocab, rules=special_cases, suffix_search=suffix_regex.search, prefix_search=prefixes_regex.search, url_match=simple_url_re.match)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVAU5JDMPhIx"
      },
      "outputs": [],
      "source": [
        "def normalization(data, regularization=\"lemma\", language='english'):\n",
        "  stopwords = nltk.corpus.stopwords.words(language)\n",
        "  ps = PorterStemmer()\n",
        "  normalized_data = []\n",
        "  \n",
        "  for tweet in data:\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # identificar retweets\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet) #eliminar links\n",
        "    tweet = re.sub(r'#', '', tweet) #eliminar símbolo gato\n",
        "    tweet = re.sub(r'@\\w+', '', tweet) #eliminar palabras que inicias con @\n",
        "    tweet = re.sub(r'\\d+', '', tweet) #eliminar números\n",
        "    tweet = re.sub(' +', ' ', tweet) #quitar espacios\n",
        "\n",
        "    if regularization == \"stem\":\n",
        "      tweetTokenizer = TweetTokenizer()\n",
        "      words = tweetTokenizer.tokenize(tweet)\n",
        "      tokens = [ps.stem(w) for w in words]\n",
        "    if regularization == \"lemma\":\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.lemma_ for token in doc]\n",
        "    else:\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.text for token in doc]\n",
        "    \n",
        "    normalized_tweets = [w for w in tokens if w not in stopwords and not w==' ' and w not in string.punctuation]\n",
        "    normalized_data.append(normalized_tweets)\n",
        "  return normalized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJjTDwD2IsyL"
      },
      "source": [
        "Create Vocabulary and frequency dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN21T4VMIsyM",
        "outputId": "d0e04491-200e-4808-b5a0-c44b43a3a072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The vocabulary has 12428 words.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "norm_pos = normalization(pos_tweets)\n",
        "norm_neg = normalization(neg_tweets)\n",
        "all_tweets = norm_pos + norm_neg\n",
        "at = [w for tweet in all_tweets for w in tweet]\n",
        "fd = nltk.FreqDist(at)\n",
        "vocabulary = sorted(list(fd.keys()))\n",
        "\n",
        "print('\\nThe vocabulary has %d words.\\n' %len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoqrk_lIsyN",
        "outputId": "914ab4f6-e9d2-47aa-e959-ec686f5f8650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_features matrix has m = 10000 examples (rows).\n",
            "\n",
            "and  n = 12429 features (columns).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_features = []\n",
        "for text in all_tweets:\n",
        "  vector = [1] # initialize in 1 ?\n",
        "  for voc in vocabulary:\n",
        "    # In vector saves a list of vocabulary's length. \n",
        "    # Iterate each vocabulary word and count in each text list\n",
        "    vector.append(text.count(voc))\n",
        "  X_features.append(vector)\n",
        "\n",
        "print('\\nX_features matrix has m = %d examples (rows).\\n' %len(X_features))\n",
        "print('and  n = %d features (columns).\\n' %len(X_features[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CKI34BB8IsyO"
      },
      "outputs": [],
      "source": [
        "tags = [1]*len(pos_tweets) + [0]*len(neg_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HF4T5yE-IsyO"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xvzq-Kk8IsyP"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2500)\n",
        "pca.fit(X_features)\n",
        "X_features_pca = pca.transform(X_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avCn0A76IsyP",
        "outputId": "52a67512-db66-40f5-e039-a2e07ab29ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perdida de información es de : 8.01 %\n"
          ]
        }
      ],
      "source": [
        "#print(pca.explained_variance_ratio_)\n",
        "n_components = 2500\n",
        "suma = np.sum(pca.explained_variance_ratio_[:n_components])\n",
        "print(\"Perdida de información es de : \" + str(round(1-suma,4)*100) + \" %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gS-11-pIsyQ"
      },
      "outputs": [],
      "source": [
        "#Principal component analysis\n",
        "def compute_pca(X, k_components=2, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Entrada:\n",
        "        X: Dimensión (m,n) donde n son las características y m son los ejemplos\n",
        "        k_components: Number of components you want to keep.\n",
        "    Salida:\n",
        "        X_reduced: Datos transformados con k_components dimensiones\n",
        "    \"\"\"\n",
        "\n",
        "    # Centrar los datos de entrada a su media\n",
        "    X_demeaned = X - np.mean(X, axis=0, keepdims=True)\n",
        "\n",
        "    # Calcula la matriz de covarianza\n",
        "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
        "\n",
        "    # Calcula los eigenvectores y eigenvalores de la matriz de covarianza\n",
        "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
        "\n",
        "    # Retorna los índices que ordenarían los eigenvalores de mayor a menor\n",
        "    idx_sorted = np.argsort(-eigen_vals)\n",
        "\n",
        "    # Ordena los eigenvalores por idx_sorted\n",
        "    eigen_vals_sorted = eigen_vals[idx_sorted]\n",
        "\n",
        "    # Ordena los eigenvectores usando los indices idx_sorted \n",
        "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted]\n",
        "\n",
        "    # Selecciona los primeros k eigenvectores\n",
        "    eigen_vecs_subset = eigen_vecs_sorted[:,0:k_components]\n",
        "\n",
        "    # Selecciona los primeros k eigenvalores\n",
        "    eigen_vals_subset = eigen_vals_sorted[:k_components]\n",
        "\n",
        "    # Calcula la pérdida de información (Varianza), un numero aceptable es < 0.01\n",
        "    variance = 1 - (sum(eigen_vals_subset) / sum(eigen_vals))\n",
        "    if variance <= tolerance:\n",
        "        print(\"Pérdida de información aceptable\")\n",
        "    else:\n",
        "        print(\"PÉRDIDA DE INFORMACIÓN IMPORTANTE - AUMENTA K-COMPONENTS\")\n",
        "\n",
        "    print(\"Porcentaje de pérdida de información del \" + str(round(1 - (variance*100), 4)) + \"%\")\n",
        "\n",
        "    # Transforma los datos, por la multiplicación de la transpuesta de los eigenvectores\n",
        "    # con la transpuesta de los datos de entrada centrados a su media\n",
        "    X_reduced = np.matmul(eigen_vecs_subset.T, X_demeaned.T)\n",
        "    X_reduced = X_reduced.T\n",
        "\n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtYg4X_LIsyQ"
      },
      "outputs": [],
      "source": [
        "X_features_pca = compute_pca(X_features, k_components=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CXiYaXcGIsyR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "doN_I9AcIsyR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_features_pca,tags,test_size=0.2, random_state=50)\n",
        "target_names = ['class 0', 'class 1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYvJSvldIsyS",
        "outputId": "e2cffe26-2a98-4a66-b5b5-5eaacc1a833c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.946\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9390    0.9540    0.9465      1001\n",
            "     class 1     0.9532    0.9379    0.9455       999\n",
            "\n",
            "    accuracy                         0.9460      2000\n",
            "   macro avg     0.9461    0.9460    0.9460      2000\n",
            "weighted avg     0.9461    0.9460    0.9460      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con DecisionTree\n",
        "modeloDT = DecisionTreeClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloDT.fit(X_train, y_train)\n",
        "predicciones = modeloDT.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloDT.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGU9PiXdIsyS",
        "outputId": "ba2012b5-0589-463f-b4ca-ce26e77dabcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.9505\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9669    0.9331    0.9497      1001\n",
            "     class 1     0.9352    0.9680    0.9513       999\n",
            "\n",
            "    accuracy                         0.9505      2000\n",
            "   macro avg     0.9510    0.9505    0.9505      2000\n",
            "weighted avg     0.9511    0.9505    0.9505      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con RandomForest\n",
        "modeloRF = RandomForestClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloRF.fit(X_train, y_train)\n",
        "predicciones = modeloRF.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloRF.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6_7IzcVIsyT",
        "outputId": "cdf64619-867b-40a0-8904-0172b8b2cf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.961\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9763    0.9451    0.9604      1001\n",
            "     class 1     0.9467    0.9770    0.9616       999\n",
            "\n",
            "    accuracy                         0.9610      2000\n",
            "   macro avg     0.9615    0.9610    0.9610      2000\n",
            "weighted avg     0.9615    0.9610    0.9610      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con SVC\n",
        "modeloSVC = SVC(kernel='linear', random_state=42)\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloSVC.fit(X_train, y_train)\n",
        "predicciones = modeloSVC.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloSVC.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyhJEhbqIsyT",
        "outputId": "f68641b4-59ba-4a12-b476-d4df7822c93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.9525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9567    0.9481    0.9523      1001\n",
            "     class 1     0.9484    0.9570    0.9527       999\n",
            "\n",
            "    accuracy                         0.9525      2000\n",
            "   macro avg     0.9525    0.9525    0.9525      2000\n",
            "weighted avg     0.9525    0.9525    0.9525      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con AdaBoostClassifier\n",
        "modeloADA = AdaBoostClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloADA.fit(X_train, y_train)\n",
        "predicciones = modeloADA.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloADA.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OESvzCmUIsyU",
        "outputId": "6f730b3e-508b-4b78-b152-0f603ee418c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.9375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9252    0.9520    0.9385      1001\n",
            "     class 1     0.9505    0.9229    0.9365       999\n",
            "\n",
            "    accuracy                         0.9375      2000\n",
            "   macro avg     0.9379    0.9375    0.9375      2000\n",
            "weighted avg     0.9379    0.9375    0.9375      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador con KNeighborsClassifier\n",
        "modeloKN = KNeighborsClassifier()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modeloKN.fit(X_train, y_train)\n",
        "predicciones = modeloKN.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modeloKN.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}