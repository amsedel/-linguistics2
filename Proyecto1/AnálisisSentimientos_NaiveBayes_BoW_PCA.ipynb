{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwN1T2dG-P39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5796d5-38fd-4d28-8d5a-74bf526d8dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk   \n",
        "import spacy            \n",
        "import re     \n",
        "import string            \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import twitter_samples    # Corpus Twitter\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rf1sRZeM3Iz"
      },
      "source": [
        "Lectura de Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBrs994RL_OH",
        "outputId": "9a811b75-19ad-41e2-c7e4-3c32c32952b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7j31D_0MDA2",
        "outputId": "c9338b25-3acc-42a8-ffcb-abfe165179d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive tweets:  5000\n",
            "Negative tweets:  5000\n"
          ]
        }
      ],
      "source": [
        "pos_tweets = twitter_samples.strings('positive_tweets.json') #tweets positivos\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json') #tweets negativos\n",
        "\n",
        "print(\"Positive tweets: \", len(pos_tweets))\n",
        "print(\"Negative tweets: \", len(neg_tweets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-An61QZMyWy"
      },
      "source": [
        "Procesamiento\n",
        "\n",
        "\n",
        "1. LowerCase\n",
        "2. Lematización / Stemming\n",
        "3. Remover stopword\n",
        "4. Remover signos de puntuación\n",
        "4. Remover urls y manejadores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0UcFgdoY9YTm"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(nlp):\n",
        "    special_cases = {\":)\": [{\"ORTH\": \":)\"}], \":(\": [{\"ORTH\": \":(\"}]}\n",
        "    simple_url_re = re.compile(r'''^https?://''')\n",
        "    suffixes = nlp.Defaults.suffixes + [r'''-+$''',]\n",
        "    prefixes = nlp.Defaults.prefixes + [r'^[\\-\\—\\–\\+\\+\\.\\!\\/\\,\\\"\\(\\)\\[\\]\\{\\}\\:\\;\\<\\>\\?\\¿\\¡\\|\\&\\#\\@\\$\\%\\^\\*\\_\\\\\\'\\`\\~]']\n",
        "    suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
        "    prefixes_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "    return spacy.tokenizer.Tokenizer(nlp.vocab, rules=special_cases, suffix_search=suffix_regex.search, prefix_search=prefixes_regex.search, url_match=simple_url_re.match)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVAU5JDMPhIx"
      },
      "outputs": [],
      "source": [
        "def normalization(data, regularization=\"lemma\", language='english'):\n",
        "  stopwords = nltk.corpus.stopwords.words(language)\n",
        "  ps = PorterStemmer()\n",
        "  normalized_data = []\n",
        "  \n",
        "  for tweet in data:\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # identificar retweets\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet) #eliminar links\n",
        "    tweet = re.sub(r'#', '', tweet) #eliminar símbolo gato\n",
        "    tweet = re.sub(r'@\\w+', '', tweet) #eliminar palabras que inicias con @\n",
        "    tweet = re.sub(r'\\d+', '', tweet) #eliminar números\n",
        "    tweet = re.sub(' +', ' ', tweet) #quitar espacios\n",
        "\n",
        "    if regularization == \"stem\":\n",
        "      tweetTokenizer = TweetTokenizer()\n",
        "      words = tweetTokenizer.tokenize(tweet)\n",
        "      tokens = [ps.stem(w) for w in words]\n",
        "    if regularization == \"lemma\":\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.lemma_ for token in doc]\n",
        "    else:\n",
        "      doc = nlp(tweet)\n",
        "      tokens = [token.text for token in doc]\n",
        "    \n",
        "    normalized_tweets = [w for w in tokens if w not in stopwords and not w==' ' and w not in string.punctuation]\n",
        "    normalized_data.append(normalized_tweets)\n",
        "  return normalized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWzi5jwn9YTo"
      },
      "source": [
        "Create Vocabulary and frequency dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ld4laV9YTp",
        "outputId": "ee3093d9-8aa9-41ca-ecb6-260feea379b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The vocabulary has 12428 words.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "norm_pos = normalization(pos_tweets)\n",
        "norm_neg = normalization(neg_tweets)\n",
        "all_tweets = norm_pos + norm_neg\n",
        "at = [w for tweet in all_tweets for w in tweet]\n",
        "fd = nltk.FreqDist(at)\n",
        "vocabulary = sorted(list(fd.keys()))\n",
        "\n",
        "print('\\nThe vocabulary has %d words.\\n' %len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTTGs5H9YTq",
        "outputId": "b11f7fc5-1f02-42a0-bc26-5b1d3e7b0085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_features matrix has m = 10000 examples (rows).\n",
            "\n",
            "and  n = 12428 features (columns).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_features = []\n",
        "for text in all_tweets:\n",
        "  vector = [] # initialize in 1 ?\n",
        "  for voc in vocabulary:\n",
        "    # In vector saves a list of vocabulary's length. \n",
        "    # Iterate each vocabulary word and count in each text list\n",
        "    vector.append(text.count(voc))\n",
        "  X_features.append(vector)\n",
        "\n",
        "print('\\nX_features matrix has m = %d examples (rows).\\n' %len(X_features))\n",
        "print('and  n = %d features (columns).\\n' %len(X_features[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9HXRY-Zc9YTr"
      },
      "outputs": [],
      "source": [
        "tags = [1]*len(pos_tweets) + [0]*len(neg_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "Cl5khKK8BlQA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2500)\n",
        "pca.fit(X_features)\n",
        "X_features_pca = pca.transform(X_features)"
      ],
      "metadata": {
        "id": "HJRV9tN-BKFD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(pca.explained_variance_ratio_)\n",
        "n_components = 2500\n",
        "suma = np.sum(pca.explained_variance_ratio_[:n_components])\n",
        "print(\"Perdida de información es de : \" + str(round(1-suma,4)*100) + \" %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrHy8rg-BjNh",
        "outputId": "e55ce638-c00a-4313-bbdc-5a22c4150541"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perdida de información es de : 8.01 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ae3M8Uj79YTr"
      },
      "outputs": [],
      "source": [
        "#Principal component analysis\n",
        "def compute_pca(X, k_components=2, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Entrada:\n",
        "        X: Dimensión (m,n) donde n son las características y m son los ejemplos\n",
        "        k_components: Number of components you want to keep.\n",
        "    Salida:\n",
        "        X_reduced: Datos transformados con k_components dimensiones\n",
        "    \"\"\"\n",
        "\n",
        "    # Centrar los datos de entrada a su media\n",
        "    X_demeaned = X - np.mean(X, axis=0, keepdims=True)\n",
        "\n",
        "    # Calcula la matriz de covarianza\n",
        "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
        "\n",
        "    # Calcula los eigenvectores y eigenvalores de la matriz de covarianza\n",
        "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
        "\n",
        "    # Retorna los índices que ordenarían los eigenvalores de mayor a menor\n",
        "    idx_sorted = np.argsort(-eigen_vals)\n",
        "\n",
        "    # Ordena los eigenvalores por idx_sorted\n",
        "    eigen_vals_sorted = eigen_vals[idx_sorted]\n",
        "\n",
        "    # Ordena los eigenvectores usando los indices idx_sorted \n",
        "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted]\n",
        "\n",
        "    # Selecciona los primeros k eigenvectores\n",
        "    eigen_vecs_subset = eigen_vecs_sorted[:,0:k_components]\n",
        "\n",
        "    # Selecciona los primeros k eigenvalores\n",
        "    eigen_vals_subset = eigen_vals_sorted[:k_components]\n",
        "\n",
        "    # Calcula la pérdida de información (Varianza), un numero aceptable es < 0.01\n",
        "    variance = 1 - (sum(eigen_vals_subset) / sum(eigen_vals))\n",
        "    if variance <= tolerance:\n",
        "        print(\"Pérdida de información aceptable\")\n",
        "    else:\n",
        "        print(\"PÉRDIDA DE INFORMACIÓN IMPORTANTE - AUMENTA K-COMPONENTS\")\n",
        "\n",
        "    print(\"Porcentaje de pérdida de información del \" + str(round(1 - (variance*100), 4)) + \"%\")\n",
        "\n",
        "    # Transforma los datos, por la multiplicación de la transpuesta de los eigenvectores\n",
        "    # con la transpuesta de los datos de entrada centrados a su media\n",
        "    X_reduced = np.matmul(eigen_vecs_subset.T, X_demeaned.T)\n",
        "    X_reduced = X_reduced.T\n",
        "\n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv23mFGf9YTs"
      },
      "outputs": [],
      "source": [
        "X_features_pca = compute_pca(X_features, k_components=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0e7_gi359YTt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ksrs2Wca9YTt"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_features_pca,tags,test_size=0.2, random_state=50)\n",
        "target_names = ['class 0', 'class 1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fbQ4rxF9YTu",
        "outputId": "34b285a0-69e0-4870-d676-772cbbe8d60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puntaje:  0.886\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9116    0.8551    0.8825      1001\n",
            "     class 1     0.8633    0.9169    0.8893       999\n",
            "\n",
            "    accuracy                         0.8860      2000\n",
            "   macro avg     0.8875    0.8860    0.8859      2000\n",
            "weighted avg     0.8875    0.8860    0.8859      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador de Naive Bayes\n",
        "modelo = BernoulliNB()\n",
        "# Entrenar el clasificador con los datos de entrenamiento\n",
        "modelo.fit(X_train, y_train)\n",
        "predicciones = modelo.predict(X_test)\n",
        "# Evaluamos el modelo\n",
        "puntaje = modelo.score(X_test, y_test)\n",
        "# Imprimimos el puntaje obtenido\n",
        "print(\"Puntaje: \", puntaje)\n",
        "print(classification_report(y_test, predicciones, target_names=target_names, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}